<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://imsanskar.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://imsanskar.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-13T13:13:41+00:00</updated><id>https://imsanskar.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Introduction to CUDA Programming</title><link href="https://imsanskar.github.io/blog/2024/introduction-to-cuda-programming/" rel="alternate" type="text/html" title="Introduction to CUDA Programming"/><published>2024-07-06T01:58:54+00:00</published><updated>2024-07-06T01:58:54+00:00</updated><id>https://imsanskar.github.io/blog/2024/introduction-to-cuda-programming</id><content type="html" xml:base="https://imsanskar.github.io/blog/2024/introduction-to-cuda-programming/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Introduction to Generative Adversarial Network</title><link href="https://imsanskar.github.io/blog/2023/gans/" rel="alternate" type="text/html" title="Introduction to Generative Adversarial Network"/><published>2023-07-14T12:57:00+00:00</published><updated>2023-07-14T12:57:00+00:00</updated><id>https://imsanskar.github.io/blog/2023/gans</id><content type="html" xml:base="https://imsanskar.github.io/blog/2023/gans/"><![CDATA[<h4 id="introduction">Introduction</h4> <p>Generative Adversarial Networks is an unsupervised model that generates samples indistinguishable samples from the training samples. To generate new samples, the main network, <em>generator</em> generates new sample from a latent variable (a random noise), and a second network known as discriminator is used to discriminate between the real and generated samples. The output of the discriminator is used to train the generator so that the generator can fool the discriminator by generating plausible samples.</p> <h4 id="discriminator-as-a-similarity-signal">Discriminator as a similarity signal</h4> <p>The goal of the GAN is to generate plausible samples ${x_i^f}$, that are indistinguishable from true training data \(\{x_i\}\). A new sample \(x_i^f\) is generated by feeding the latent variable \(z_i\) from a simple base distribution to the <em>generator network</em>, \(g(z_i, \phi)\) with parameters \(\phi\). The learning process involves the optimization of variable \(\phi\) so that generated samples \(\{x_i^f\}\) is similar to \(\{x_i\}\). To quantify the notion of plausibility, GAN uses an additional network a <em>discriminator</em>, \(f(x, \theta)\) to classify whether an image is generated by a generator or from true samples. The discriminator is used to provide a signal that can be used to improve the generation process.</p> <h4 id="loss-function">Loss function</h4> <p>The discriminator is trained to optimize a binary classification task, whether the data is from the training set or generated by a generator. The discriminator maximizes the probability of assigning the correct label to both training examples and samples obtained from the generator i.e. maximize \(log(D(x)) + log (1 - D(G(z)))\). However, during implementation, we minimize the traditional binary classification problem:</p> \[\theta^* = \underset{\theta}{\operatorname{argmin}} \sum_i- y_i \text{ } log f(x, \theta) - (1 - y_i) \text{ } log (1 - f(g(z, \phi), \theta))\] <p>where, \(y_i \in \{0, 1\}\) is the label. We assume that real examples from the training dataset have label \(y = 1\) and the generated sample have label \(y = 0\), then the above equation can be written as:</p> \[\theta^* = \underset{\theta}{\operatorname{argmin}} \sum_i- log \text{ } f(x, \theta) - \text{ } log (1 - f(g(z, \phi), \theta))\] <p>In contrast with the discriminator, the generator maximizes the negative log probability of score predicted by the discriminator \(log(1 - f(g(z, \phi), \theta))\) i.e. it seeks to generate a sample that is misclassified by the discriminator. Now, the overall loss function can be formulated as:</p> <p>\(\phi^*, \theta^* = \underset{\phi}{\operatorname{argmax}} \{ \underset{\theta}{\operatorname{min}} \sum_i- log \text{ } f(x, \theta) - \text{ } log (1 - f(g(z, \phi), \theta)) \}\) </p> <h4 id="implementation-of-gan">Implementation of GAN</h4> <p>In this section, we implement a simple GAN network in PyTorch. We generate data by sampling from a mixture of Gaussian distribution. We optimize GAN to generate data that mimics the initial distribution of the dataset.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/data_distribution-480.webp 480w,/assets/img/data_distribution-800.webp 800w,/assets/img/data_distribution-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/data_distribution.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We create two multi-layer perceptron networks for the generator and discriminator and use SGD optimizer to optimize the loss function for each generator and discriminator simultaneously. We define the networks for both the generator and discriminator. The code for training GAN is provided below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
		<span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>		
		<span class="n">self</span><span class="p">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
		<span class="n">self</span><span class="p">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>	
		<span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
		<span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>		
		<span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">weight</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div> <p>Now, we implement the training loop for the generator and discriminator.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">generator_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">discriminator_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">valid</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">fake</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer_generator</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x_f</span> <span class="o">=</span> <span class="nf">generator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">prediction_discriminator</span> <span class="o">=</span> <span class="nf">discriminator</span><span class="p">(</span><span class="n">x_f</span><span class="p">)</span>
        <span class="n">loss_generator</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">prediction_discriminator</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">loss_generator</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer_generator</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">generator_loss</span> <span class="o">+=</span> <span class="n">loss_generator</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

        <span class="n">optimizer_discriminator</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">prediction_generated</span> <span class="o">=</span> <span class="nf">discriminator</span><span class="p">(</span><span class="n">x_f</span><span class="p">.</span><span class="nf">detach</span><span class="p">())</span>
        <span class="n">prediction_true</span> <span class="o">=</span> <span class="nf">discriminator</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss_discriminator</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">prediction_generated</span><span class="p">,</span> <span class="n">fake</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">)</span> <span class="o">+</span> \
                                <span class="n">F</span><span class="p">.</span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">prediction_true</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">loss_discriminator</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer_discriminator</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">discriminator_loss</span> <span class="o">+=</span> <span class="n">loss_discriminator</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> Loss: (</span><span class="si">{</span><span class="n">generator_loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">discriminator_loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>After training the GAN, we visualize the actual distribution and the distribution of samples generated by the generator. As we can see from the figure below that the synthetic data samples generated almost mimics the actual distribution of the dataset.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/generated_data_distribution-480.webp 480w,/assets/img/generated_data_distribution-800.webp 800w,/assets/img/generated_data_distribution-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/generated_data_distribution.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Generative"/><category term="Networks"/><category term="GAN"/><summary type="html"><![CDATA[Introduction Generative Adversarial Networks is an unsupervised model that generates samples indistinguishable samples from the training samples. To generate new samples, the main network, generator generates new sample from a latent variable (a random noise), and a second network known as discriminator is used to discriminate between the real and generated samples. The output of the discriminator is used to train the generator so that the generator can fool the discriminator by generating plausible samples.]]></summary></entry><entry><title type="html">Templates and Compile Time Execution</title><link href="https://imsanskar.github.io/blog/2023/templates-and-compile-time-execution/" rel="alternate" type="text/html" title="Templates and Compile Time Execution"/><published>2023-01-08T14:42:46+00:00</published><updated>2023-01-08T14:42:46+00:00</updated><id>https://imsanskar.github.io/blog/2023/templates-and-compile-time-execution</id><content type="html" xml:base="https://imsanskar.github.io/blog/2023/templates-and-compile-time-execution/"><![CDATA[]]></content><author><name></name></author></entry></feed>